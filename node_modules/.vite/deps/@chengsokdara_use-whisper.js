import {
  require_react
} from "./chunk-JFTBQ7A7.js";
import {
  __toESM
} from "./chunk-AC2VUBZ6.js";

// node_modules/@chengsokdara/use-whisper/dist/chunk-VO7VPLVP.js
var t = "https://unpkg.com/@ffmpeg/core-st@0.11.1/dist/ffmpeg-core.js";
var o = "silenceremove=start_periods=1:stop_periods=-1:start_threshold=-30dB:stop_threshold=-30dB:start_silence=2:stop_silence=2";
var s = "https://api.openai.com/v1/audio/";

// node_modules/@chengsokdara/react-hooks-async/dist/chunk-AMUF5NKT.js
var import_react = __toESM(require_react(), 1);

// node_modules/@chengsokdara/react-hooks-async/dist/chunk-LBCDGULV.js
var import_react2 = __toESM(require_react(), 1);
var u = (i2, e, n, o3) => {
  let y3 = (0, import_react2.useMemo)(() => {
    let t2 = [];
    return Array.isArray(e) && (t2 = [...t2, ...e]), Array.isArray(n) && (t2 = [...t2, ...n]), Array.isArray(o3) && (t2 = [...t2, ...o3]), t2;
  }, [o3, e, n]);
  return (0, import_react2.useEffect)(() => ((async () => {
    try {
      await i2();
    } catch (t2) {
      typeof n == "function" ? n(t2) : console.info(t2);
    }
  })(), () => {
    typeof e == "function" && e();
  }), y3);
};

// node_modules/@chengsokdara/react-hooks-async/dist/chunk-YORICPLC.js
var import_react3 = __toESM(require_react(), 1);
var a = (c2, n, t2) => {
  let i2 = (0, import_react3.useMemo)(() => {
    let e = [];
    return Array.isArray(n) && (e = [...e, ...n]), Array.isArray(t2) && (e = [...e, ...t2]), e;
  }, [n, t2]);
  return (0, import_react3.useMemo)(() => async (...e) => {
    try {
      return await c2(...e);
    } catch (o3) {
      typeof n == "function" ? n(o3) : console.error("useMemo", o3);
    }
  }, i2);
};

// node_modules/@chengsokdara/use-whisper/dist/chunk-32KRFHOA.js
var import_react4 = __toESM(require_react(), 1);
var ne = { apiKey: "", autoStart: false, autoTranscribe: true, mode: "transcriptions", nonStop: false, removeSilence: false, stopTimeout: 5e3, streaming: false, timeSlice: 1e3, onDataAvailable: void 0, onTranscribe: void 0 };
var oe = { stop: void 0 };
var ae = { blob: void 0, text: void 0 };
var ue = (P) => {
  let { apiKey: m, autoStart: A, autoTranscribe: U, mode: h, nonStop: B, removeSilence: M, stopTimeout: q, streaming: S, timeSlice: I, whisperConfig: c$1, onDataAvailable: K, onTranscribe: T } = { ...ne, ...P };
  if (!m && !T)
    throw new Error("apiKey is required if onTranscribe is not provided");
  let f = (0, import_react4.useRef)([]), i2 = (0, import_react4.useRef)(), s2 = (0, import_react4.useRef)(), t2 = (0, import_react4.useRef)(), a2 = (0, import_react4.useRef)(), d$1 = (0, import_react4.useRef)(oe), [O, k] = (0, import_react4.useState)(false), [$, C] = (0, import_react4.useState)(false), [j, g] = (0, import_react4.useState)(false), [z, l2] = (0, import_react4.useState)(ae);
  (0, import_react4.useEffect)(() => () => {
    f.current && (f.current = []), i2.current && (i2.current.flush(), i2.current = void 0), t2.current && (t2.current.destroy(), t2.current = void 0), b$1("stop"), s2.current && (s2.current.off("speaking", R), s2.current.off("stopped_speaking", v)), a2.current && (a2.current.getTracks().forEach((e) => e.stop()), a2.current = void 0);
  }, []), u(async () => {
    A && await W();
  }, [A]);
  let N = async () => {
    await W();
  }, G = async () => {
    await V();
  }, J = async () => {
    await E();
  }, W = async () => {
    try {
      if (a2.current || await Q(), a2.current) {
        if (!t2.current) {
          let { default: { RecordRTCPromisesHandler: r2, StereoAudioRecorder: o3 } } = await import("./RecordRTC-WHJ4WPZ3.js"), n = { mimeType: "audio/wav", numberOfAudioChannels: 1, recorderType: o3, sampleRate: 44100, timeSlice: S ? I : void 0, type: "audio", ondataavailable: U && S ? Z : void 0 };
          t2.current = new r2(a2.current, n);
        }
        if (!i2.current) {
          let { Mp3Encoder: r2 } = await import("./js-63BSVS6Y.js");
          i2.current = new r2(1, 44100, 96);
        }
        let e = await t2.current.getState();
        (e === "inactive" || e === "stopped") && await t2.current.startRecording(), e === "paused" && await t2.current.resumeRecording(), B && x("stop"), k(true);
      }
    } catch {
    }
  }, Q = async () => {
    try {
      if (a2.current && a2.current.getTracks().forEach((e) => e.stop()), a2.current = await navigator.mediaDevices.getUserMedia({ audio: true }), !s2.current) {
        let { default: e } = await import("./hark-CXWT4PVK.js");
        s2.current = e(a2.current, { interval: 100, play: false }), s2.current.on("speaking", R), s2.current.on("stopped_speaking", v);
      }
    } catch {
    }
  }, x = (e) => {
    d$1.current[e] || (d$1.current[e] = setTimeout(E, q));
  }, R = () => {
    C(true), b$1("stop");
  }, v = () => {
    C(false), B && x("stop");
  }, V = async () => {
    try {
      t2.current && (await t2.current.getState() === "recording" && await t2.current.pauseRecording(), b$1("stop"), k(false));
    } catch {
    }
  }, E = async () => {
    try {
      if (t2.current) {
        let e = await t2.current.getState();
        if ((e === "recording" || e === "paused") && await t2.current.stopRecording(), X(), b$1("stop"), k(false), U)
          await Y();
        else {
          let r2 = await t2.current.getBlob();
          l2({ blob: r2 });
        }
        await t2.current.destroy(), f.current = [], i2.current && (i2.current.flush(), i2.current = void 0), t2.current = void 0;
      }
    } catch {
    }
  }, X = () => {
    s2.current && (s2.current.off("speaking", R), s2.current.off("stopped_speaking", v), s2.current = void 0), a2.current && (a2.current.getTracks().forEach((e) => e.stop()), a2.current = void 0);
  }, b$1 = (e) => {
    d$1.current[e] && (clearTimeout(d$1.current[e]), d$1.current[e] = void 0);
  }, Y = async () => {
    try {
      if (i2.current && t2.current && await t2.current.getState() === "stopped") {
        g(true);
        let r2 = await t2.current.getBlob();
        if (M) {
          let { createFFmpeg: o3 } = await import("./src-7YBNFUWF.js"), n = o3({ mainName: "main", corePath: t, log: true });
          n.isLoaded() || await n.load();
          let u2 = await r2.arrayBuffer();
          n.FS("writeFile", "in.wav", new Uint8Array(u2)), await n.run("-i", "in.wav", "-acodec", "libmp3lame", "-b:a", "96k", "-ar", "44100", "-af", o, "out.mp3");
          let w = n.FS("readFile", "out.mp3");
          if (w.length <= 225) {
            n.exit(), l2({ blob: r2 }), g(false);
            return;
          }
          r2 = new Blob([w.buffer], { type: "audio/mpeg" }), n.exit();
        } else {
          let o3 = await r2.arrayBuffer(), n = i2.current.encodeBuffer(new Int16Array(o3));
          r2 = new Blob([n], { type: "audio/mpeg" });
        }
        if (typeof T == "function") {
          let o3 = await T(r2);
          l2(o3);
        } else {
          let o3 = new File([r2], "speech.mp3", { type: "audio/mpeg" }), n = await F(o3);
          l2({ blob: r2, text: n });
        }
        g(false);
      }
    } catch {
      g(false);
    }
  }, Z = async (e) => {
    try {
      if (S && t2.current) {
        if (K == null ? void 0 : K(e), i2.current) {
          let o3 = await e.arrayBuffer(), n = i2.current.encodeBuffer(new Int16Array(o3)), u2 = new Blob([n], { type: "audio/mpeg" });
          f.current.push(u2);
        }
        if (await t2.current.getState() === "recording") {
          let o3 = new Blob(f.current, { type: "audio/mpeg" }), n = new File([o3], "speech.mp3", { type: "audio/mpeg" }), u2 = await F(n);
          u2 && l2((w) => ({ ...w, text: u2 }));
        }
      }
    } catch {
    }
  }, F = a(async (e) => {
    let r2 = new FormData();
    r2.append("file", e), r2.append("model", "whisper-1"), h === "transcriptions" && r2.append("language", (c$1 == null ? void 0 : c$1.language) ?? "en"), (c$1 == null ? void 0 : c$1.prompt) && r2.append("prompt", c$1.prompt), (c$1 == null ? void 0 : c$1.response_format) && r2.append("response_format", c$1.response_format), (c$1 == null ? void 0 : c$1.temperature) && r2.append("temperature", `${c$1.temperature}`);
    let o3 = {};
    o3["Content-Type"] = "multipart/form-data", m && (o3.Authorization = `Bearer ${m}`);
    let { default: n } = await import("./axios.js");
    return (await n.post(s + h, r2, { headers: o3 })).data.text;
  }, [m, h, c$1]);
  return { recording: O, speaking: $, transcribing: j, transcript: z, pauseRecording: G, startRecording: N, stopRecording: J };
};

// node_modules/@chengsokdara/use-whisper/dist/index.js
var o2 = ue;
export {
  o2 as default,
  ue as useWhisper
};
//# sourceMappingURL=@chengsokdara_use-whisper.js.map
